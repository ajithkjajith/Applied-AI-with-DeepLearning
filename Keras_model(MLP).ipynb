{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "#create a Multi-layer perceptron (MLP) using Keras\n# This dataset consists of 11,228 newswires from the Reuters news agency,Each wire is encoded as a sequence of word indexes"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "!pip install tensorflow==2.2.0rc0"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "import tensorflow as tf\nif not tf.__version__ == '2.2.0-rc0':\n    print(tf.__version__)\n    raise ValueError('please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "IMPORTANT! => Please restart the kernel by clicking on \"Kernel\"->\"Restart and Clear Outout\" and wait until all output disapears. Then your changes are beeing picked up\n\nAs you can see, we use Keras' Sequential model with only two types of layers: Dense and Dropout. We also specify a random seed to make our results reproducible. Next, we load the Reuters data set:"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n2113536/2110848 [==============================] - 0s 0us/step\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n/opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
                }
            ],
            "source": "import numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\nseed = 1337\nnp.random.seed(seed)\nfrom tensorflow.keras.datasets import reuters\n\nmax_words = 1000\n(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n                                                         test_split=0.2,\n                                                         seed=seed)\nnum_classes = np.max(y_train) + 1  # 46 topics"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "from tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=max_words)\nx_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\nx_test = tokenizer.sequences_to_matrix(x_test, mode='binary')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\nUse to_categorical, as we did in the lectures, to transform both *y_train* and *y_test* into one-hot encoded vectors of length *num_classes*:"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "y_train = to_categorical(y_train,num_classes)\ny_test = to_categorical(y_test,num_classes)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n\n    Layer: Add a *Dense* layer with in input_shape=(max_words,), 512 output units and \"relu\" activation.\n    Layer: Add a *Dropout* layer with dropout rate of 50%.\n    Layer: Add a *Dense* layer with num_classes output units and \"softmax\" activation."
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "model = Sequential()\nmodel.add(Dense(512,activation = \"relu\",input_shape=(max_words,)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes,activation=\"softmax\"))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "compiling"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "model training and evaluation\n"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/5\n281/281 [==============================] - 1s 5ms/step - loss: 1.3937 - accuracy: 0.6830\nEpoch 2/5\n281/281 [==============================] - 1s 5ms/step - loss: 0.7831 - accuracy: 0.8176\nEpoch 3/5\n281/281 [==============================] - 1s 5ms/step - loss: 0.5437 - accuracy: 0.8678\nEpoch 4/5\n281/281 [==============================] - 2s 5ms/step - loss: 0.4207 - accuracy: 0.8943\nEpoch 5/5\n281/281 [==============================] - 2s 6ms/step - loss: 0.3352 - accuracy: 0.9144\n71/71 [==============================] - 0s 2ms/step - loss: 0.8360 - accuracy: 0.8072\n"
                }
            ],
            "source": "batch_size = 32\nmodel.fit(x_train,y_train,batch_size = batch_size, epochs=5)\nscore = model.evaluate(x_test,y_test)"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.8072128295898438"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "score[1]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}